{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["cJ3DIXv02qnv"]},"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"language_info":{"name":"python","version":"3.9.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"cells":[{"cell_type":"markdown","source":["#Introduction"],"metadata":{"id":"j2HuYdmm2V-q"}},{"cell_type":"code","execution_count":null,"source":["!pip install torch\n","!pip install tokenizers\n","!pip install transformers"],"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in ./anaconda3/lib/python3.9/site-packages (1.13.0)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./anaconda3/lib/python3.9/site-packages (from torch) (11.10.3.66)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./anaconda3/lib/python3.9/site-packages (from torch) (8.5.0.96)\n","Requirement already satisfied: typing-extensions in ./anaconda3/lib/python3.9/site-packages (from torch) (4.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./anaconda3/lib/python3.9/site-packages (from torch) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./anaconda3/lib/python3.9/site-packages (from torch) (11.7.99)\n","Requirement already satisfied: wheel in ./anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n","Requirement already satisfied: setuptools in ./anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (63.4.1)\n","Requirement already satisfied: tokenizers in ./anaconda3/lib/python3.9/site-packages (0.13.2)\n","Requirement already satisfied: transformers in ./anaconda3/lib/python3.9/site-packages (4.24.0)\n","Requirement already satisfied: numpy>=1.17 in ./anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n","Requirement already satisfied: pyyaml>=5.1 in ./anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in ./anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in ./anaconda3/lib/python3.9/site-packages (from transformers) (0.11.1)\n","Requirement already satisfied: filelock in ./anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./anaconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n","Requirement already satisfied: tqdm>=4.27 in ./anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n","Requirement already satisfied: requests in ./anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n","Requirement already satisfied: packaging>=20.0 in ./anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n","Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.14)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n","Requirement already satisfied: charset-normalizer<3,>=2 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u5p1LmXdSR86","outputId":"a4287f0f-81e1-4ac1-8180-4db9d9cd641d"}},{"cell_type":"code","execution_count":null,"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"],"outputs":[],"metadata":{"id":"t--cEvFZWP-V"}},{"cell_type":"code","execution_count":null,"source":["path = '/content/drive/MyDrive/CSE676DeepLearning/raw_bangla_for_BERT.txt'"],"outputs":[],"metadata":{"id":"wd6aPH0jXaO0"}},{"cell_type":"code","execution_count":null,"source":["from tokenizers import BertWordPieceTokenizer\n","\n","# initialize\n","tokenizer = BertWordPieceTokenizer()\n","\n","# and train\n","tokenizer.train(files=path, \n","                vocab_size=50_000, \n","                min_frequency=2,\n","                limit_alphabet=1000, \n","                wordpieces_prefix='##',\n","                special_tokens=[\n","                    '[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])"],"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n"]}],"metadata":{"id":"ZUWIsFdCWDH5","outputId":"2ef55c62-a4b1-4b96-8ce3-c3adbede1363"}},{"cell_type":"code","execution_count":null,"source":["# Train a tokenizer\n","import tokenizers\n"," \n","bertToken= tokenizers.BertWordPieceTokenizer()\n"," \n","path = \"/content/drive/MyDrive/CSE676DeepLearning/raw_bangla_for_BERT.txt\"\n","\n","bertToken.train(\n","    files=path,\n","    vocab_size=50000,\n","    min_frequency=3,\n","    limit_alphabet=1000\n",")"],"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n"]}],"metadata":{"id":"wMPE3dQGSsXQ","outputId":"d3d67a55-8dd6-49c5-83f2-66cca0a6d4de"}},{"cell_type":"code","execution_count":null,"source":["bertToken.save_model('/content')"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/home/shruti/vocab.txt']"]},"metadata":{},"execution_count":5}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eaL3WQaMYe8c","outputId":"4bb56837-ba23-4fd1-99dc-9ad6e40d5403"}},{"cell_type":"code","execution_count":null,"source":["# Load the tokenizer\n","from transformers import BertTokenizer, LineByLineTextDataset\n","\n","file_dir = '/content/drive/MyDrive/CSE676DeepLearning/vocab.txt\n","\n","tokenizer = BertTokenizer.from_pretrained(file_dir)\n","\n","sentence = '‡¶∂‡ßá‡¶∑ ‡¶¶‡¶ø‡¶ï‡ßá ‡¶∏‡ßá‡¶®‡¶æ‡¶¨‡¶æ‡¶π‡¶ø‡¶®‡ßÄ‡¶∞ ‡¶∏‡¶¶‡¶∏‡ßç‡¶Ø‡¶∞‡¶æ ‡¶è‡¶∏‡¶¨ ‡¶ò‡¶∞ ‡¶§‡¶æ‡¶Å‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡¶æ‡¶∏‡¶®‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶π‡¶∏‡ßç‡¶§‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡ßá‡¶®'\n","\n","enc_input = tokenizer.tokenize(sentence)\n","print(enc_input)"],"outputs":[{"output_type":"stream","name":"stdout","text":["['‡¶∂‡ßá‡¶∑', '‡¶¶‡¶ø‡¶ï‡ßá', '‡¶∏‡ßá‡¶®‡¶æ‡¶¨‡¶æ‡¶π‡¶ø‡¶®‡ßÄ‡¶∞', '‡¶∏‡¶¶‡¶∏‡¶Ø‡¶∞‡¶æ', '‡¶è‡¶∏‡¶¨', '‡¶ò‡¶∞', '‡¶§‡¶æ‡¶∞', '‡¶™‡¶∞‡¶∂‡¶æ‡¶∏‡¶®‡ßá‡¶∞', '‡¶ï‡¶æ‡¶õ‡ßá', '‡¶π‡¶∏‡¶§‡¶æ‡¶®‡¶§‡¶∞', '‡¶ï‡¶∞‡ßá‡¶®']\n"]},{"output_type":"stream","name":"stderr","text":["/home/shruti/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1679: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n","  warnings.warn(\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O4VWY4XfaLW0","outputId":"0dcdffe6-1033-42a5-9cfa-b43bf310aac7"}},{"cell_type":"code","execution_count":null,"source":["%time\n","\n","dataset = LineByLineTextDataset(\n","    tokenizer = tokenizer,\n","    file_path = '/content/drive/MyDrive/CSE676DeepLearning/raw_bangla_for_BERT.txt',\n","    block_size = 128  # maximum sequence length\n",")\n","\n","print('No. of lines: ', len(dataset)) # No of lines in your datset"],"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 5 ¬µs, sys: 0 ns, total: 5 ¬µs\n","Wall time: 10.7 ¬µs\n"]},{"output_type":"stream","name":"stderr","text":["/home/shruti/anaconda3/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["No. of lines:  2172033\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Nw6xZWObSAV","outputId":"2f62fa71-4ae9-4e3b-e0ff-3db877846f26"}},{"cell_type":"code","execution_count":null,"source":["from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n","\n","config = BertConfig(\n","    vocab_size=50000,\n","    hidden_size=768, \n","    num_hidden_layers=6, \n","    num_attention_heads=12,\n","    max_position_embeddings=512\n",")\n"," \n","model = BertForMaskedLM(config)\n","\n","print('No of parameters: ', model.num_parameters())\n","\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"],"outputs":[{"output_type":"stream","name":"stdout","text":["No of parameters:  81965648\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75YbTp33bynG","outputId":"af93d7a2-0100-4660-c59c-4e9b78b25eac"}},{"cell_type":"code","execution_count":null,"source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir='/content/drive/MyDrive/CSE676DeepLearning/',\n","    overwrite_output_dir=True,\n","    num_train_epochs=1,\n","    per_device_train_batch_size=32,\n","    save_steps=10_000,\n","    save_total_limit=2\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset  \n",")"],"outputs":[],"metadata":{"id":"D3je71FSgfiE"}},{"cell_type":"code","execution_count":null,"source":["%%time\n","trainer.train()"],"outputs":[{"output_type":"stream","name":"stderr","text":["/home/shruti/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 2172033\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 67877\n","  Number of trainable parameters = 81965648\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='67877' max='67877' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [67877/67877 1:03:38, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>9.264100</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>8.789200</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>8.493300</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>8.355900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>8.190200</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>8.083800</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>7.981500</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>7.931500</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>7.863800</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>7.749700</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>7.624100</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>7.581300</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>7.499500</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>7.370500</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>7.337200</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>7.239700</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>7.194000</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>7.070000</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>7.032800</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>6.986900</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>6.914200</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>6.858200</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>6.794500</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>6.731500</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>6.719800</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>6.634100</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>6.563800</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>6.566500</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>6.500300</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>6.474100</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>6.431900</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>6.435700</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>6.374200</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>6.338600</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>6.222300</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>6.275600</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>6.193900</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>6.198200</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>6.189900</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>6.155700</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>6.135900</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>6.141000</td>\n","    </tr>\n","    <tr>\n","      <td>21500</td>\n","      <td>6.054000</td>\n","    </tr>\n","    <tr>\n","      <td>22000</td>\n","      <td>6.073000</td>\n","    </tr>\n","    <tr>\n","      <td>22500</td>\n","      <td>6.049400</td>\n","    </tr>\n","    <tr>\n","      <td>23000</td>\n","      <td>5.988200</td>\n","    </tr>\n","    <tr>\n","      <td>23500</td>\n","      <td>5.986100</td>\n","    </tr>\n","    <tr>\n","      <td>24000</td>\n","      <td>5.983700</td>\n","    </tr>\n","    <tr>\n","      <td>24500</td>\n","      <td>5.895900</td>\n","    </tr>\n","    <tr>\n","      <td>25000</td>\n","      <td>5.914600</td>\n","    </tr>\n","    <tr>\n","      <td>25500</td>\n","      <td>5.865900</td>\n","    </tr>\n","    <tr>\n","      <td>26000</td>\n","      <td>5.839000</td>\n","    </tr>\n","    <tr>\n","      <td>26500</td>\n","      <td>5.851500</td>\n","    </tr>\n","    <tr>\n","      <td>27000</td>\n","      <td>5.861700</td>\n","    </tr>\n","    <tr>\n","      <td>27500</td>\n","      <td>5.805100</td>\n","    </tr>\n","    <tr>\n","      <td>28000</td>\n","      <td>5.820300</td>\n","    </tr>\n","    <tr>\n","      <td>28500</td>\n","      <td>5.793300</td>\n","    </tr>\n","    <tr>\n","      <td>29000</td>\n","      <td>5.739100</td>\n","    </tr>\n","    <tr>\n","      <td>29500</td>\n","      <td>5.681800</td>\n","    </tr>\n","    <tr>\n","      <td>30000</td>\n","      <td>5.745000</td>\n","    </tr>\n","    <tr>\n","      <td>30500</td>\n","      <td>5.702000</td>\n","    </tr>\n","    <tr>\n","      <td>31000</td>\n","      <td>5.697100</td>\n","    </tr>\n","    <tr>\n","      <td>31500</td>\n","      <td>5.692600</td>\n","    </tr>\n","    <tr>\n","      <td>32000</td>\n","      <td>5.651000</td>\n","    </tr>\n","    <tr>\n","      <td>32500</td>\n","      <td>5.644600</td>\n","    </tr>\n","    <tr>\n","      <td>33000</td>\n","      <td>5.629300</td>\n","    </tr>\n","    <tr>\n","      <td>33500</td>\n","      <td>5.614300</td>\n","    </tr>\n","    <tr>\n","      <td>34000</td>\n","      <td>5.607400</td>\n","    </tr>\n","    <tr>\n","      <td>34500</td>\n","      <td>5.585400</td>\n","    </tr>\n","    <tr>\n","      <td>35000</td>\n","      <td>5.567200</td>\n","    </tr>\n","    <tr>\n","      <td>35500</td>\n","      <td>5.561400</td>\n","    </tr>\n","    <tr>\n","      <td>36000</td>\n","      <td>5.559500</td>\n","    </tr>\n","    <tr>\n","      <td>36500</td>\n","      <td>5.530600</td>\n","    </tr>\n","    <tr>\n","      <td>37000</td>\n","      <td>5.536700</td>\n","    </tr>\n","    <tr>\n","      <td>37500</td>\n","      <td>5.528200</td>\n","    </tr>\n","    <tr>\n","      <td>38000</td>\n","      <td>5.502500</td>\n","    </tr>\n","    <tr>\n","      <td>38500</td>\n","      <td>5.488800</td>\n","    </tr>\n","    <tr>\n","      <td>39000</td>\n","      <td>5.467000</td>\n","    </tr>\n","    <tr>\n","      <td>39500</td>\n","      <td>5.503600</td>\n","    </tr>\n","    <tr>\n","      <td>40000</td>\n","      <td>5.463100</td>\n","    </tr>\n","    <tr>\n","      <td>40500</td>\n","      <td>5.475100</td>\n","    </tr>\n","    <tr>\n","      <td>41000</td>\n","      <td>5.454700</td>\n","    </tr>\n","    <tr>\n","      <td>41500</td>\n","      <td>5.467200</td>\n","    </tr>\n","    <tr>\n","      <td>42000</td>\n","      <td>5.368500</td>\n","    </tr>\n","    <tr>\n","      <td>42500</td>\n","      <td>5.426200</td>\n","    </tr>\n","    <tr>\n","      <td>43000</td>\n","      <td>5.395400</td>\n","    </tr>\n","    <tr>\n","      <td>43500</td>\n","      <td>5.423800</td>\n","    </tr>\n","    <tr>\n","      <td>44000</td>\n","      <td>5.437600</td>\n","    </tr>\n","    <tr>\n","      <td>44500</td>\n","      <td>5.393100</td>\n","    </tr>\n","    <tr>\n","      <td>45000</td>\n","      <td>5.373700</td>\n","    </tr>\n","    <tr>\n","      <td>45500</td>\n","      <td>5.372000</td>\n","    </tr>\n","    <tr>\n","      <td>46000</td>\n","      <td>5.392700</td>\n","    </tr>\n","    <tr>\n","      <td>46500</td>\n","      <td>5.367700</td>\n","    </tr>\n","    <tr>\n","      <td>47000</td>\n","      <td>5.331400</td>\n","    </tr>\n","    <tr>\n","      <td>47500</td>\n","      <td>5.365400</td>\n","    </tr>\n","    <tr>\n","      <td>48000</td>\n","      <td>5.314500</td>\n","    </tr>\n","    <tr>\n","      <td>48500</td>\n","      <td>5.313800</td>\n","    </tr>\n","    <tr>\n","      <td>49000</td>\n","      <td>5.304100</td>\n","    </tr>\n","    <tr>\n","      <td>49500</td>\n","      <td>5.306800</td>\n","    </tr>\n","    <tr>\n","      <td>50000</td>\n","      <td>5.307000</td>\n","    </tr>\n","    <tr>\n","      <td>50500</td>\n","      <td>5.282500</td>\n","    </tr>\n","    <tr>\n","      <td>51000</td>\n","      <td>5.270500</td>\n","    </tr>\n","    <tr>\n","      <td>51500</td>\n","      <td>5.208200</td>\n","    </tr>\n","    <tr>\n","      <td>52000</td>\n","      <td>5.250900</td>\n","    </tr>\n","    <tr>\n","      <td>52500</td>\n","      <td>5.260700</td>\n","    </tr>\n","    <tr>\n","      <td>53000</td>\n","      <td>5.249200</td>\n","    </tr>\n","    <tr>\n","      <td>53500</td>\n","      <td>5.247400</td>\n","    </tr>\n","    <tr>\n","      <td>54000</td>\n","      <td>5.218600</td>\n","    </tr>\n","    <tr>\n","      <td>54500</td>\n","      <td>5.226600</td>\n","    </tr>\n","    <tr>\n","      <td>55000</td>\n","      <td>5.230800</td>\n","    </tr>\n","    <tr>\n","      <td>55500</td>\n","      <td>5.215000</td>\n","    </tr>\n","    <tr>\n","      <td>56000</td>\n","      <td>5.212600</td>\n","    </tr>\n","    <tr>\n","      <td>56500</td>\n","      <td>5.276600</td>\n","    </tr>\n","    <tr>\n","      <td>57000</td>\n","      <td>5.185800</td>\n","    </tr>\n","    <tr>\n","      <td>57500</td>\n","      <td>5.206800</td>\n","    </tr>\n","    <tr>\n","      <td>58000</td>\n","      <td>5.183900</td>\n","    </tr>\n","    <tr>\n","      <td>58500</td>\n","      <td>5.249900</td>\n","    </tr>\n","    <tr>\n","      <td>59000</td>\n","      <td>5.171600</td>\n","    </tr>\n","    <tr>\n","      <td>59500</td>\n","      <td>5.232400</td>\n","    </tr>\n","    <tr>\n","      <td>60000</td>\n","      <td>5.182000</td>\n","    </tr>\n","    <tr>\n","      <td>60500</td>\n","      <td>5.215600</td>\n","    </tr>\n","    <tr>\n","      <td>61000</td>\n","      <td>5.126300</td>\n","    </tr>\n","    <tr>\n","      <td>61500</td>\n","      <td>5.177000</td>\n","    </tr>\n","    <tr>\n","      <td>62000</td>\n","      <td>5.188500</td>\n","    </tr>\n","    <tr>\n","      <td>62500</td>\n","      <td>5.178900</td>\n","    </tr>\n","    <tr>\n","      <td>63000</td>\n","      <td>5.138900</td>\n","    </tr>\n","    <tr>\n","      <td>63500</td>\n","      <td>5.198200</td>\n","    </tr>\n","    <tr>\n","      <td>64000</td>\n","      <td>5.171300</td>\n","    </tr>\n","    <tr>\n","      <td>64500</td>\n","      <td>5.166800</td>\n","    </tr>\n","    <tr>\n","      <td>65000</td>\n","      <td>5.152200</td>\n","    </tr>\n","    <tr>\n","      <td>65500</td>\n","      <td>5.140600</td>\n","    </tr>\n","    <tr>\n","      <td>66000</td>\n","      <td>5.134200</td>\n","    </tr>\n","    <tr>\n","      <td>66500</td>\n","      <td>5.160500</td>\n","    </tr>\n","    <tr>\n","      <td>67000</td>\n","      <td>5.130500</td>\n","    </tr>\n","    <tr>\n","      <td>67500</td>\n","      <td>5.186800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /home/shruti/checkpoint-10000\n","Configuration saved in /home/shruti/checkpoint-10000/config.json\n","Model weights saved in /home/shruti/checkpoint-10000/pytorch_model.bin\n","Saving model checkpoint to /home/shruti/checkpoint-20000\n","Configuration saved in /home/shruti/checkpoint-20000/config.json\n","Model weights saved in /home/shruti/checkpoint-20000/pytorch_model.bin\n","Deleting older checkpoint [/home/shruti/checkpoint-60000] due to args.save_total_limit\n","Saving model checkpoint to /home/shruti/checkpoint-30000\n","Configuration saved in /home/shruti/checkpoint-30000/config.json\n","Model weights saved in /home/shruti/checkpoint-30000/pytorch_model.bin\n","Deleting older checkpoint [/home/shruti/checkpoint-10000] due to args.save_total_limit\n","Saving model checkpoint to /home/shruti/checkpoint-40000\n","Configuration saved in /home/shruti/checkpoint-40000/config.json\n","Model weights saved in /home/shruti/checkpoint-40000/pytorch_model.bin\n","Deleting older checkpoint [/home/shruti/checkpoint-20000] due to args.save_total_limit\n","Saving model checkpoint to /home/shruti/checkpoint-50000\n","Configuration saved in /home/shruti/checkpoint-50000/config.json\n","Model weights saved in /home/shruti/checkpoint-50000/pytorch_model.bin\n","Deleting older checkpoint [/home/shruti/checkpoint-30000] due to args.save_total_limit\n","Saving model checkpoint to /home/shruti/checkpoint-60000\n","Configuration saved in /home/shruti/checkpoint-60000/config.json\n","Model weights saved in /home/shruti/checkpoint-60000/pytorch_model.bin\n","Deleting older checkpoint [/home/shruti/checkpoint-40000] due to args.save_total_limit\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 1h 3min 29s, sys: 5.3 s, total: 1h 3min 34s\n","Wall time: 1h 3min 38s\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=67877, training_loss=5.9554953764599015, metrics={'train_runtime': 3818.6179, 'train_samples_per_second': 568.801, 'train_steps_per_second': 17.775, 'total_flos': 2.495318058611789e+16, 'train_loss': 5.9554953764599015, 'epoch': 1.0})"]},"metadata":{},"execution_count":11}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":612},"id":"hJ5gLF5MlIIt","outputId":"8ebe28fd-a160-4963-b762-899f023f410a"}},{"cell_type":"code","execution_count":null,"source":["trainer.save_model('/home/shruti/')"],"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /home/shruti/\n","Configuration saved in /home/shruti/config.json\n","Model weights saved in /home/shruti/pytorch_model.bin\n"]}],"metadata":{"id":"71hTGV212UTy","outputId":"cbad058b-7c69-42a5-bec1-7ef119c63b7c"}},{"cell_type":"code","execution_count":null,"source":["from transformers import pipeline\n","\n","model = BertForMaskedLM.from_pretrained('/home/shruti/')\n","\n","fill_mask = pipeline(\n","    \"fill-mask\",\n","    model=model,\n","    tokenizer=tokenizer\n",")"],"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file /home/shruti/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.24.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50000\n","}\n","\n","loading weights file /home/shruti/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForMaskedLM.\n","\n","All the weights of BertForMaskedLM were initialized from the model checkpoint at /home/shruti/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"]}],"metadata":{"id":"rm1J5Y4NlYyN","outputId":"f828d903-5086-452b-b4f2-857eccce42e3"}},{"cell_type":"code","execution_count":null,"source":["fill_mask('‡¶≤‡¶æ‡¶∂ ‡¶â‡¶¶‡ßç‡¶ß‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶Æ‡¶Ø‡¶º‡¶®‡¶æ‡¶§‡¶¶‡¶®‡ßç‡¶§‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡¶ï‡ßç‡¶∏‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞ [MASK] ‡¶Æ‡¶∞‡ßç‡¶ó‡ßá ‡¶™‡¶æ‡¶†‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá ‡¶™‡ßÅ‡¶≤‡¶ø‡¶∂')"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.277895450592041,\n","  'token': 3119,\n","  'token_str': '‡¶π‡¶æ‡¶∏‡¶™‡¶æ‡¶§‡¶æ‡¶≤',\n","  'sequence': '‡¶≤‡¶æ‡¶∂ ‡¶â‡¶¶‡¶ß‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶Æ‡¶Ø‡¶®‡¶æ‡¶§‡¶¶‡¶®‡¶§‡ßá‡¶∞ ‡¶ú‡¶®‡¶Ø ‡¶ï‡¶ï‡¶∏‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶π‡¶æ‡¶∏‡¶™‡¶æ‡¶§‡¶æ‡¶≤ ‡¶Æ‡¶∞‡¶ó‡ßá ‡¶™‡¶æ‡¶†‡¶ø‡¶Ø‡ßá‡¶õ‡ßá ‡¶™‡¶≤‡¶ø‡¶∂'},\n"," {'score': 0.20703396201133728,\n","  'token': 1408,\n","  'token_str': '‡¶∏‡¶¶‡¶∞',\n","  'sequence': '‡¶≤‡¶æ‡¶∂ ‡¶â‡¶¶‡¶ß‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶Æ‡¶Ø‡¶®‡¶æ‡¶§‡¶¶‡¶®‡¶§‡ßá‡¶∞ ‡¶ú‡¶®‡¶Ø ‡¶ï‡¶ï‡¶∏‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶∏‡¶¶‡¶∞ ‡¶Æ‡¶∞‡¶ó‡ßá ‡¶™‡¶æ‡¶†‡¶ø‡¶Ø‡ßá‡¶õ‡ßá ‡¶™‡¶≤‡¶ø‡¶∂'},\n"," {'score': 0.19087183475494385,\n","  'token': 1894,\n","  'token_str': '‡¶Æ‡ßá‡¶°‡¶ø‡¶ï‡ßá‡¶≤',\n","  'sequence': '‡¶≤‡¶æ‡¶∂ ‡¶â‡¶¶‡¶ß‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶Æ‡¶Ø‡¶®‡¶æ‡¶§‡¶¶‡¶®‡¶§‡ßá‡¶∞ ‡¶ú‡¶®‡¶Ø ‡¶ï‡¶ï‡¶∏‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶Æ‡ßá‡¶°‡¶ø‡¶ï‡ßá‡¶≤ ‡¶Æ‡¶∞‡¶ó‡ßá ‡¶™‡¶æ‡¶†‡¶ø‡¶Ø‡ßá‡¶õ‡ßá ‡¶™‡¶≤‡¶ø‡¶∂'},\n"," {'score': 0.054660357534885406,\n","  'token': 3740,\n","  'token_str': '‡¶π‡¶æ‡¶∏‡¶™‡¶æ‡¶§‡¶æ‡¶≤‡ßá‡¶∞',\n","  'sequence': '‡¶≤‡¶æ‡¶∂ ‡¶â‡¶¶‡¶ß‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶Æ‡¶Ø‡¶®‡¶æ‡¶§‡¶¶‡¶®‡¶§‡ßá‡¶∞ ‡¶ú‡¶®‡¶Ø ‡¶ï‡¶ï‡¶∏‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶π‡¶æ‡¶∏‡¶™‡¶æ‡¶§‡¶æ‡¶≤‡ßá‡¶∞ ‡¶Æ‡¶∞‡¶ó‡ßá ‡¶™‡¶æ‡¶†‡¶ø‡¶Ø‡ßá‡¶õ‡ßá ‡¶™‡¶≤‡¶ø‡¶∂'},\n"," {'score': 0.03167051821947098,\n","  'token': 2340,\n","  'token_str': '‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶≤',\n","  'sequence': '‡¶≤‡¶æ‡¶∂ ‡¶â‡¶¶‡¶ß‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶Æ‡¶Ø‡¶®‡¶æ‡¶§‡¶¶‡¶®‡¶§‡ßá‡¶∞ ‡¶ú‡¶®‡¶Ø ‡¶ï‡¶ï‡¶∏‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶≤ ‡¶Æ‡¶∞‡¶ó‡ßá ‡¶™‡¶æ‡¶†‡¶ø‡¶Ø‡ßá‡¶õ‡ßá ‡¶™‡¶≤‡¶ø‡¶∂'}]"]},"metadata":{},"execution_count":15}],"metadata":{"id":"d1Eyv-cQ_Nt2","outputId":"4d234d72-13f1-4cd7-b5fc-ade1afc4b849"}},{"cell_type":"code","execution_count":null,"source":["fill_mask('‡ßß‡ßØ‡ß≠‡ßß ‡¶∏‡¶æ‡¶≤‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡ßØ ‡¶Æ‡¶æ‡¶∏ ‡¶Æ‡ßÅ‡¶ï‡ßç‡¶§‡¶ø‡¶Ø‡ßÅ‡¶¶‡ßç‡¶ß ‡¶ï‡¶∞‡ßá [MASK] ‡¶Ö‡¶∞‡ßç‡¶ú‡¶® ‡¶ï‡¶∞‡ßá')"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.08184559643268585,\n","  'token': 1853,\n","  'token_str': '‡¶™‡¶∞‡¶∏‡¶ï‡¶æ‡¶∞',\n","  'sequence': '‡ßß‡ßØ‡ß≠‡ßß ‡¶∏‡¶æ‡¶≤‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡ßØ ‡¶Æ‡¶æ‡¶∏ ‡¶Æ‡¶ï‡¶§‡¶ø‡¶Ø‡¶¶‡¶ß ‡¶ï‡¶∞‡ßá ‡¶™‡¶∞‡¶∏‡¶ï‡¶æ‡¶∞ ‡¶Ö‡¶∞‡¶ú‡¶® ‡¶ï‡¶∞‡ßá'},\n"," {'score': 0.036910369992256165,\n","  'token': 2587,\n","  'token_str': '‡¶∏‡¶¨‡¶æ‡¶ß‡ßÄ‡¶®‡¶§‡¶æ',\n","  'sequence': '‡ßß‡ßØ‡ß≠‡ßß ‡¶∏‡¶æ‡¶≤‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡ßØ ‡¶Æ‡¶æ‡¶∏ ‡¶Æ‡¶ï‡¶§‡¶ø‡¶Ø‡¶¶‡¶ß ‡¶ï‡¶∞‡ßá ‡¶∏‡¶¨‡¶æ‡¶ß‡ßÄ‡¶®‡¶§‡¶æ ‡¶Ö‡¶∞‡¶ú‡¶® ‡¶ï‡¶∞‡ßá'},\n"," {'score': 0.022689618170261383,\n","  'token': 787,\n","  'token_str': '‡¶Æ‡¶ï‡¶§‡¶ø',\n","  'sequence': '‡ßß‡ßØ‡ß≠‡ßß ‡¶∏‡¶æ‡¶≤‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡ßØ ‡¶Æ‡¶æ‡¶∏ ‡¶Æ‡¶ï‡¶§‡¶ø‡¶Ø‡¶¶‡¶ß ‡¶ï‡¶∞‡ßá ‡¶Æ‡¶ï‡¶§‡¶ø ‡¶Ö‡¶∞‡¶ú‡¶® ‡¶ï‡¶∞‡ßá'},\n"," {'score': 0.02130352146923542,\n","  'token': 316,\n","  'token_str': '‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂',\n","  'sequence': '‡ßß‡ßØ‡ß≠‡ßß ‡¶∏‡¶æ‡¶≤‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡ßØ ‡¶Æ‡¶æ‡¶∏ ‡¶Æ‡¶ï‡¶§‡¶ø‡¶Ø‡¶¶‡¶ß ‡¶ï‡¶∞‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡¶Ö‡¶∞‡¶ú‡¶® ‡¶ï‡¶∞‡ßá'},\n"," {'score': 0.019313856959342957,\n","  'token': 303,\n","  'token_str': '‡¶¶‡ßá‡¶∂',\n","  'sequence': '‡ßß‡ßØ‡ß≠‡ßß ‡¶∏‡¶æ‡¶≤‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡ßØ ‡¶Æ‡¶æ‡¶∏ ‡¶Æ‡¶ï‡¶§‡¶ø‡¶Ø‡¶¶‡¶ß ‡¶ï‡¶∞‡ßá ‡¶¶‡ßá‡¶∂ ‡¶Ö‡¶∞‡¶ú‡¶® ‡¶ï‡¶∞‡ßá'}]"]},"metadata":{},"execution_count":16}],"metadata":{"id":"c1zHZ1i-_Vj8","outputId":"30590966-59fc-41d7-ddcb-e5af5663656c"}},{"cell_type":"markdown","source":["#ALBERT"],"metadata":{"id":"cJ3DIXv02qnv"}},{"cell_type":"code","execution_count":null,"source":["!pip install sentencepiece"],"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentencepiece in ./anaconda3/lib/python3.9/site-packages (0.1.97)\n"]}],"metadata":{"id":"7dsraVFA278W","outputId":"6a14f64a-be4b-4260-c3b2-a1b1e388e4c9"}},{"cell_type":"code","execution_count":null,"source":["#from sentencepiece import sentencepiece_pb2\n","import sentencepiece as spm\n","from transformers import AlbertConfig\n","from transformers import LineByLineTextDataset"],"outputs":[],"metadata":{"id":"-zOa3Kc9291z"}},{"cell_type":"code","execution_count":null,"source":["#Download sentencepiece model\n","!wget \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v2-spiece.model\"\n","!wget \"https://raw.githubusercontent.com/google/sentencepiece/master/python/src/sentencepiece/sentencepiece_pb2.py\"\n","!wget https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb\n","!wget \"https://huggingface.co/albert-base-v2/raw/main/config.json\""],"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-11-30 22:31:27--  https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v2-spiece.model\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.137.88, 52.217.192.200, 52.217.205.32, ...\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.137.88|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 760289 (742K) [binary/octet-stream]\n","Saving to: ‚Äòalbert-base-v2-spiece.model.3‚Äô\n","\n","albert-base-v2-spie 100%[===================>] 742.47K  --.-KB/s    in 0.1s    \n","\n","2022-11-30 22:31:27 (5.88 MB/s) - ‚Äòalbert-base-v2-spiece.model.3‚Äô saved [760289/760289]\n","\n","--2022-11-30 22:31:27--  https://raw.githubusercontent.com/google/sentencepiece/master/python/src/sentencepiece/sentencepiece_pb2.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 8128 (7.9K) [text/plain]\n","Saving to: ‚Äòsentencepiece_pb2.py.3‚Äô\n","\n","sentencepiece_pb2.p 100%[===================>]   7.94K  --.-KB/s    in 0.001s  \n","\n","2022-11-30 22:31:27 (13.4 MB/s) - ‚Äòsentencepiece_pb2.py.3‚Äô saved [8128/8128]\n","\n","--2022-11-30 22:31:27--  https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb\n","Resolving github.com (github.com)... 140.82.114.3\n","Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‚Äòsentencepiece_python_module_example.ipynb.3‚Äô\n","\n","sentencepiece_pytho     [ <=>                ] 149.91K  --.-KB/s    in 0.04s   \n","\n","2022-11-30 22:31:28 (4.15 MB/s) - ‚Äòsentencepiece_python_module_example.ipynb.3‚Äô saved [153512]\n","\n","--2022-11-30 22:31:28--  https://huggingface.co/albert-base-v2/raw/main/config.json\n","Resolving huggingface.co (huggingface.co)... 54.147.99.175, 34.227.196.80, 2600:1f18:147f:e850:fad3:e054:c752:ff16, ...\n","Connecting to huggingface.co (huggingface.co)|54.147.99.175|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 684 [text/plain]\n","Saving to: ‚Äòconfig.json.4‚Äô\n","\n","config.json.4       100%[===================>]     684  --.-KB/s    in 0s      \n","\n","2022-11-30 22:31:28 (646 MB/s) - ‚Äòconfig.json.4‚Äô saved [684/684]\n","\n"]}],"metadata":{"id":"TiNi-V8625kq","outputId":"1636e784-ac7e-4a37-8060-21ef019b83cd"}},{"cell_type":"code","execution_count":null,"source":["spiece_model_path = 'albert-base-v2-spiece.model'\n","albert_config_path = 'config.json'\n","albert_model_path = 'albert-base-v2'"],"outputs":[],"metadata":{"id":"8WkCZldk3EFj"}},{"cell_type":"code","execution_count":null,"source":["sp = spm.SentencePieceProcessor()\n","sp.load(spiece_model_path)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":22}],"metadata":{"id":"zlDXPiW13G-D","outputId":"bac90462-2dd1-466f-e1b2-b4717927909b"}},{"cell_type":"code","execution_count":null,"source":["!pip install transformers"],"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in ./anaconda3/lib/python3.9/site-packages (4.24.0)\n","Requirement already satisfied: packaging>=20.0 in ./anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in ./anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./anaconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n","Requirement already satisfied: filelock in ./anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n","Requirement already satisfied: tqdm>=4.27 in ./anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in ./anaconda3/lib/python3.9/site-packages (from transformers) (0.11.1)\n","Requirement already satisfied: pyyaml>=5.1 in ./anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in ./anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n","Requirement already satisfied: numpy>=1.17 in ./anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.14)\n","Requirement already satisfied: charset-normalizer<3,>=2 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n","Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n"]}],"metadata":{"id":"Jm_gIh5X3JMG","outputId":"f67a3849-e2aa-4f01-bbc5-c63abf4c3d93"}},{"cell_type":"code","execution_count":null,"source":["#Load and check tokenizer\n","#import tensorflow as tf\n","from transformers import AlbertTokenizer, TFAlbertForMaskedLM\n","\n","#tokenizer = AlbertTokenizer()\n","\n","albert_tokenizer = AlbertTokenizer.from_pretrained(spiece_model_path, do_lower_case=True)\n","albert_tokenizer.tokenize(\"Test tokenizer\")"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["['‚ñÅtest', '‚ñÅto', 'ken', 'izer']"]},"metadata":{},"execution_count":24}],"metadata":{"id":"TPK5R9kl3L_E","outputId":"7c995e07-cdc9-4355-dc0e-5392a5c59789"}},{"cell_type":"code","execution_count":null,"source":["sp.encode_as_pieces(\"Test tokenizer\".lower())"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["['‚ñÅtest', '‚ñÅto', 'ken', 'izer']"]},"metadata":{},"execution_count":25}],"metadata":{"id":"wkzIpxPm3Z0l","outputId":"4f29c4b6-9348-476a-fd6b-8b4d832c0c06"}},{"cell_type":"code","execution_count":null,"source":["class OffsetTokenizer():\n","    def __init__(self, path_model=spiece_model_path):\n","        self.spt = sentencepiece_pb2.SentencePieceText()\n","        self.sp = spm.SentencePieceProcessor()\n","        self.sp.load(path_model)\n","        \n","    def encode(self, text, return_tokens=False, lower=True):\n","        if lower:\n","            text = text.lower()\n","        offset = []\n","        ids = []\n","        self.spt.ParseFromString(self.sp.encode_as_serialized_proto(text))\n","        \n","        for piece in self.spt.pieces:\n","            offset.append((piece.begin, piece.end))\n","            ids.append(piece.id)\n","            \n","        if return_tokens:\n","            return sp.encode_as_pieces(text), ids, offset\n","        else:\n","            return ids, offset"],"outputs":[],"metadata":{"id":"O5m_K_0x3dPl"}},{"cell_type":"code","execution_count":null,"source":["#o_tokenizer = OffsetTokenizer()\n","#o_tokenizer.encode(\"Test tokenizer\", return_tokens=False)"],"outputs":[],"metadata":{"id":"dNcou1bn3hsL"}},{"cell_type":"code","execution_count":null,"source":["albert_config = AlbertConfig.from_pretrained(albert_config_path)\n","albert_config.output_hidden_states=True"],"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type bert to instantiate a model of type albert. This is not supported for all configurations of models and can yield errors.\n"]}],"metadata":{"id":"JUyIVml03mFN","outputId":"db599240-6493-4e9c-db21-74d5d5cfa621"}},{"cell_type":"code","execution_count":null,"source":["%time\n","\n","aldataset = LineByLineTextDataset(\n","    tokenizer = albert_tokenizer,\n","    file_path = path,\n","    block_size = 64\n",")\n","\n","print('No. of lines: ', len(aldataset)) # No of lines in your datset"],"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 4 ¬µs, sys: 0 ns, total: 4 ¬µs\n","Wall time: 7.63 ¬µs\n","No. of lines:  2172033\n"]}],"metadata":{"id":"FQ7iqUoH3pxX","outputId":"d1a8747f-fe7f-4e8f-8c8a-c431d4eab303"}},{"cell_type":"code","execution_count":null,"source":["from transformers import AlbertConfig, AlbertForMaskedLM, DataCollatorForLanguageModeling\n","\n","alconfig = AlbertConfig()\n"," \n","almodel = AlbertForMaskedLM(alconfig)\n","\n","print('No of parameters: ', almodel.num_parameters())\n","\n","aldata_collator = DataCollatorForLanguageModeling(tokenizer=albert_tokenizer, mlm=True, mlm_probability=0.15)"],"outputs":[{"output_type":"stream","name":"stdout","text":["No of parameters:  206368944\n"]}],"metadata":{"id":"FtsJ-WWo3uCI","outputId":"e1d24b88-0c60-44a8-84b5-1cb74804a297"}},{"cell_type":"code","execution_count":null,"source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir='/home/shruti/',\n","    overwrite_output_dir=True,\n","    num_train_epochs=1,\n","    per_device_train_batch_size=32,\n","    save_steps=10_000,\n","    save_total_limit=2\n",")\n","\n","altrainer = Trainer(\n","    model=almodel,\n","    args=training_args,\n","    data_collator=aldata_collator,\n","    train_dataset=aldataset,    \n",")"],"outputs":[],"metadata":{"id":"ZB4ach7_3xCL"}},{"cell_type":"code","execution_count":null,"source":["%%time\n","altrainer.train()"],"outputs":[{"output_type":"stream","name":"stderr","text":["/home/shruti/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 2172033\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 67877\n","  Number of trainable parameters = 206368944\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1278' max='67877' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 1278/67877 29:58 < 26:04:22, 0.71 it/s, Epoch 0.02/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>5.998700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.876400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}],"metadata":{"id":"jwr4kQHE3zz5","outputId":"4e8ab6bd-e4e4-4bc5-a1d1-ab2db2f1e73d"}},{"cell_type":"code","execution_count":null,"source":["altrainer.save_model('/content/drive/MyDrive/CSE676DeepLearning/')"],"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'altrainer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_758267/3286173312.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maltrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/CSE676DeepLearning/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'altrainer' is not defined"]}],"metadata":{"id":"VfJU0_-z31dI","outputId":"e307cfbb-5c24-4a05-8510-07d2e41103a4"}},{"cell_type":"code","execution_count":null,"source":["from transformers import pipeline\n","\n","model = AlbertForMaskedLM.from_pretrained('/content/drive/MyDrive/CSE676DeepLearning/')\n","\n","fill_mask2 = pipeline(\n","    \"fill-mask\",\n","     model=almodel,\n","     tokenizer=albert_tokenizer\n",")"],"outputs":[],"metadata":{"id":"wVA__yuu33dT"}},{"cell_type":"code","execution_count":null,"source":["fill_mask2('‡¶≤‡¶æ‡¶∂ ‡¶â‡¶¶‡ßç‡¶ß‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶Æ‡¶Ø‡¶º‡¶®‡¶æ‡¶§‡¶¶‡¶®‡ßç‡¶§‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡¶ï‡ßç‡¶∏‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞ [MASK] ‡¶Æ‡¶∞‡ßç‡¶ó‡ßá ‡¶™‡¶æ‡¶†‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá ‡¶™‡ßÅ‡¶≤‡¶ø‡¶∂')"],"outputs":[],"metadata":{"id":"4x3VA0Fw36OO"}}]}